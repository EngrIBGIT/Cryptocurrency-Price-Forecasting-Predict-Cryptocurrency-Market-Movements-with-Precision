# -*- coding: utf-8 -*-
"""Cryptocurrency Price Forecasting with Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kT_UyG8vFn5V0ioJhmsxV0wiHlU0YXJt

# Project Title: Cryptocurrency Price Forecasting with Machine Learning

This project aims to successfully forecast cryptocurrency prices for the purpose of financial responsibility.

---

## 2. Methodology

This project will be carried out using the __CRISP-DM__ methodology. This is one of the more popular data science methodologies and is characterized by six important phases, which are as follows:

1. _Business Understanding_,
2. _Data Understanding_,
3. _Data Preparation_,
4. _Data Modelling_,
5. _Model Evaluation_, and
6. _Model Deployment_.

It should be noted that these phases are usually recurrent in nature (i.e., some phases may be repeated). As such, they do not necessarily follow a linear progression.

---

## 3. Tools

The tools of use for this project include:

1. _Pandas_
2. _NumPy_
3. _Matplotlib_ & _Seaborn_
4. _Statsmodel_

#### 3.1. Pandas & NumPy

__Pandas__ is a Python library built upon the __NumPy__ library. The idea behind _Pandas_ is to be able to operate on text data, where _NumPy_ is best suited for numerical operations, irrespective of the fact that it can represent text to some degree.

#### 3.2. Matplotlib & Seaborn

__Matplotlib__ and __Seaborn__ are Python libraries for data visualization. Other alternatives include __Bokeh__ and __Plotly__.

#### 3.3. Statsmodel

__Statsmodel__ is a Python library for statistical data analysis and modeling. Unlike libraries like __Sci-kit Learn__, which is focused on more algorithmic models. data visualization.

---

<div align="center"><h1>Project Implementation via CRISP-DM</h1></div>

---

<div><h3>01. Business Understanding</h3></div>

__Quantum Investments__ is a well-known quantitative trading firm famed for its cutting-edge quantitative trading strategies and data-driven investment decisions. They aim to improve their financial decisions and operations by the application of machine learning to the task of forecasting cryptocurrency prices.g.

This would be an immense boon to them in the following ways:

1. _Provide insight into possible future directions of market prices_,
2. _Obtain an understanding of the important predictors of said market prices_.

In order to train the needed machine learning models, the IT team at _Quantum Investments_ will need data that records and describes the state of different cryptocurrency over time. Some of this information would include __market capitalization__, __cryptocurrency prices__, __total market value__ etcetera.

---

<div><h3>02. Data Understanding</h3></div>

With the _**Business Understanding**_ out of the way, the next step is to understand the data to be obtained and used for the task. This will involve the process of __*Exploratory Data Analysis (EDA)*__.

EDA is the process of sifting through data with the goal of extracting insights. These insights allow a better understanding of the available data and what can be done with it. They can also be used for guided preparation of the dataset in the appropriate manner. Just like regular analysis, EDA begins with a set of __questions__ and/or __hypotheses__. The EDA process will then prove or disprove these hypotheses, and hopefully, reveal other points of inquiry along the way.

The required libraries and packages are imported first. The EDA process is carried out here as shown below. The high-level steps to follow are:

1. Import the required libraries
2. Load in the dataset
3. Analyze and observe its properties.
   * Missing data
   * Inconsistent values
   * Low categorical cardinality
   * Feature correlations
   * Time series stationarity
4. Report on these properties and how they might affect our final solution.

<div align="center"><h3>2.1. Enter EDA Code Here</h3></div>
"""

# Import required utilities
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

sns.set()

from sklearn.neighbors import LocalOutlierFactor

# Seed for reproducibility
SEED = np.random.seed(seed = 2023)

# Import the dataset
data_path = "Cryptocurrency_History_Data.arff"

def truncate_data(data):
    if data.isdigit():
        if "." in data:
            return float(data)
        return int(data)

    if "-" in data:
        data = data.replace(" ", "")

    return data

def truncate_row(row):
    return [truncate_data(d) for d in row]

def read_data(path):
    with open(path, "r") as f:
        lines = f.readlines()

    ix = 0
    for (i, line) in enumerate(lines):
        if line.__contains__("@DATA"):
            ix = i
            break

    header = [l.split(" ")[1] for l in lines[:ix] if l.startswith("@ATTRIBUTE")]

    data = [truncate_row(l.replace("\n", "").strip().split(",")) for l in lines[ix+1:]]
    data = [l for l in data if len(l) == len(header)]


    # for i, line in enumerate(data):
    #     if len(line) != 17:
    #         print("Index:", i)

    return pd.DataFrame(data = data, columns = header)

data = read_data(data_path)

data.head(n = 5)

data.info()

"""##### Cardinality"""

# Feature cardinality
data.nunique()

data.shape

"""#### Data summary"""

# Describe data
data.describe(include="all")

"""##### Missing Values"""

# Check for missing values
100 * data.isnull().sum() / len(data)

"""Based on the results of the data summary and the missing values. It would seem that the missing values are represented by ``?``. This can be verified below."""

data["platform_name"].value_counts()

data.loc[data["platform_name"] == "?"]

"""These ``?`` need to be replaced by the regular ``NaN``. This is done below:"""

data.replace("?", np.nan, inplace = True)

"""Now, with the changes made, we can confirm the status of the dataset with respect to missing values."""

100 * data.isnull().sum() / len(data)

"""As can be seen, more than half of the features are plauged with missing values. However, some features like `platform_name` and `github_link` are strongly plagued by this issue.

##### Cardinality

We can take a look at the number of values each feature can take.
"""

# Number of unique values per feature
data.nunique()

data.info()

# Distribution of crypto types
data["crypto_type"].value_counts(normalize = True)

# Distribution of crypto mineability
data["minable"].value_counts(normalize = True)

# Line chart of crypto prices over time
sns.histplot(x = data["crypto_type"], bins = 10)

plt.xlabel("Time (in years)")
plt.ylabel("Crypto Price")

plt.xticks([0, 1], ["Type I", "Type II"])

plt.title("Evolution of cryptocurrency prices over time", fontsize = 20, pad = 10)

plt.legend()

plt.tight_layout()
plt.show()

"""The features with the lowest cardinality have a cardinalitty of `2`. This means there are no invariant features in the dataset. Also, the crytocurrencie are almost equally distributed between the cryptocurrency types. However, based on minability, there are very few minable crptocurrencies in the dataset.

##### Duplicate records

We eliminate duplicated rows.
"""

# Get the duplicated records
num_duplicated = len(data.loc[data.duplicated()])

print(f"Number of duplicated records: {num_duplicated}.")

"""There are very few duplicated records in the dataset. This is good, and also astounding, given that we are dealing with aroung \~ 2 million records in total."""

# Filter away the duplicated records
data = data.loc[~data.duplicated()]

int_types = ["minable", "volume", "crypto_type", "max_supply"]
float_types = [
    "price_usd", "price_btc", "market_cap", "capitalization_change_1_day",
    "USD_price_change_1_day", "BTC_price_change_1_day"
]

for column in int_types:
    # try:
    #     data[column] = pd.to_numeric(data[column], downcast = "integer", errors='coerce').astype(pd.Int64Dtype())
    # except:
    data[column] = data[column].astype("Float32")
    data[column] = data[column].astype("Int64")

for column in float_types:
    data[column] = data[column].astype(float)

data["trade_date"] = pd.to_datetime(data["trade_date"])

data.head(n = 5)

data.info()

100 * data.isnull().sum() / len(data)

"""##### Distribution of dataset between cryptocurrencies
We can observe the distribution of cryptocurrency.
"""

# Distribution of crypto coins
data["crypto_name"].value_counts()

# Number of unique crypto coins
print(len(data["crypto_name"].unique()))

# Distribution of crypto coins (Bitcoin and Ethereum)
data["crypto_name"].value_counts()[["Bitcoin", "Ethereum", "Litecoin", "Dogecoin"]]

"""We can observe that some cryptocurrency tokens are more prevalent in the dataset than others. Bitcoin and ethereum are more popular in the mainstream cryptospace. As such, actual modelling will eventually be based on one of them."""





"""##### Distribution of Cryptocurrency Price & Market Capitalization
We can observe the distribution of crypto prices and their market capitalization.
"""

# Line chart of crypto prices over time
coins_of_interest = ["Bitcoin", "Ethereum", "Litecoin", "Dogecoin"]
plt.figure(figsize = (15, 7.5))

df = data.loc[data["crypto_name"].isin(coins_of_interest)]
sns.lineplot(data = df, x = "trade_date", y = "price_usd", hue = "crypto_name")

plt.xlabel("Time (in years)")
plt.ylabel("Crypto Price")

plt.title("Evolution of cryptocurrency prices over time", fontsize = 20, pad = 10)

plt.legend()

plt.tight_layout()
plt.show()

"""From the visualization above, it is obvious that the price of Bitcoin has experienced more change over time. The prices of Ethereum, Litecoin, and Dogecoin, by contrast, have remained relatively stable over the years.

As part of __Quantum Investments__ decision-making process, it might be worth spending more time and resources focusing on Bitcoin, instead of Ethereum.
"""

# Line chart of crypto market cap over time
plt.figure(figsize = (15, 7.5))

df = data.loc[data["crypto_name"].isin(coins_of_interest)]
sns.lineplot(data = df, x = "trade_date", y = "market_cap", hue = "crypto_name")

plt.xlabel("Time (in years)")
plt.ylabel("Market Capitalization")

plt.title("Evolution of cryptocurrency market capitalization over time", fontsize = 20, pad = 10)

plt.legend()

plt.tight_layout()
plt.show()

"""The visual above shows us that both Bitcoin and Ethereum experience considerable change in their market capitalization over time. However, by the year 2021, the market capitalization of Ethereum has almost stabilized, while Bitcoin still exhibits an upward trend.

This implies that the Bitcoin market still has room for growth, while the Ethereum market is more stagnant, and may not be likely to expand so soon. The other coins show very little changes in terms of market capitalization. Also, taking the last two visualizations in tandem, it appears that the market capitalization of Ethereum changes even when the price remains mostly stagnant. This would seem to imply that most of the changes in market capitalization for Bitcoin is due to the change in prce, while that for Ethereum is mostly due to increase in the inherent value of the coin.
"""

# Line chart comparing Ethereum prices and market cap over time
plt.figure(figsize = (15, 7.5))

df = data.loc[data["crypto_name"].isin(["Ethereum", "Bitcoin"])]
df["cap_per_price"] = df["market_cap"] / df["price_usd"] # Compute market cap per price

sns.lineplot(data = df, x = "trade_date", y = "cap_per_price", hue = "crypto_name")

plt.xlabel("Time (in years)")
plt.ylabel("Market Capitalization per Price")

plt.title("Evolution of cryptocurrency market capitalization per price over time", fontsize = 20, pad = 10)

plt.legend()

plt.tight_layout()
plt.show()

# Line chart comparing Ethereum prices and market cap over time
plt.figure(figsize = (15, 7.5))

df = data.loc[data["crypto_name"].isin(["Ethereum", "Bitcoin"])]
df["cap_change_per_price_change"] = df["capitalization_change_1_day"] / df["USD_price_change_1_day"] # Compute market cap per price

sns.lineplot(data = df, x = "trade_date", y = "cap_change_per_price_change", hue = "crypto_name")

plt.xlabel("Time (in years)")
plt.ylabel("Market Capitalization")

plt.title("Evolution of cryptocurrency market capitalization over time", fontsize = 20, pad = 10)

plt.legend()

plt.tight_layout()
plt.show()

df.columns

"""##### Feature correlation

It would make some sense to observe the features for colinearity. As most of the features in the dataset are numerical, the __Pearson correlation coefficient__ will be utilized.
"""

correlation = data.select_dtypes(exclude=["object", "datetime"]).corr(method = "pearson")

correlation

# Correlation heat map

plt.figure(figsize = (15, 7.5))

sns.heatmap(correlation, center=.5, annot = True)

plt.title("Correlation heatmap", fontsize = 20, pad = 10)

plt.tight_layout()
plt.show()

"""From the visual heatmap above, there is no multicollinearity in the dataset.

From the correlation analysis above, the only tangible level of correlation is observed with the _crypto_tye_ and _minable_ columns. This makes some sense, as the type of cryptocurrency of interest will affect whether or not it is minable. Beyond that, it is obvious that most of the features exhibit low correlations with one another and with the target variable itself. This may be taken as a mix of good and bad news.

The good news is that there is little colinearity among the features. This is good for feature independence, which might imply that each variable encodes information that might be relatively orthogonal to the information encoded within other variables. The bad news is, that any model trained in these features might have a difficult time learning anything from the data, as the target variable has little correlation with the independent variables.

#### 2.2. Data Implications

Implied by our findings above, we can say the following:

1. ___Missing values and outliers___: There are a lot of missing values in the dataset. This may be fixed either by value imputation of missingness encoding. As regards outliers, there are quite a lot also. These can be filtered out.

2. ___Data duplication___: There are very few duplicated values out of more than 2 million records.

3. ___Low cardinality or feature invariance___: There are no invariant features. This means all the features in the dataset might be useful in some way. Also, most features

4. ___Correlation___: For the most part, the features exhibit very little collinearity, hence ensuring feature independence. However, the target variable exhibits a low correlation with the independent variables.


---

<div><h3>03. Data Preparation</h3></div>

Based on the *__Data Implications__* discovered prior, the following steps will be experimented upon for the data preparation stage.

1. __Missing Values__: The missing values will be treated via an algorithmic method, ___SimpleImputer___ where needed.

2. __Feature Encoding__: Depending on the final model used, the categorical features such as _platform_name_, _industry_name_, etcetera may need to be encoded.

3. __Filter Data__: For accurate and focused forecasting, the subset of data to be modelled will be __Bitcoin__ data. This is in accordance with the recommendation that the more potentially beneficial cryptocurrencies be focused upon by __Quatum Investments__.

4. __Unnecessary Features__: Depending on the model being used, some of the features are not required. These extra features will be eliminated.

<div align="center"><h3>3.1. Enter Data Preparation Code Here</h3></div>
"""

# Select Bitcoin data
df = data.loc[data["crypto_name"] == "Bitcoin", ["trade_date", "price_usd"]]

df.set_index("trade_date", inplace=True)
df.index = pd.DatetimeIndex(df.index).to_period('D')

df.head()

# df = df.resample("D").mean()

df.head()

df.isnull().sum()

df.nunique()

df.shape

"""---

<div><h3>04. Data Modelling</h3></div>
"""

# from sklearn.model_selection import train_test_split

# from sklearn.impute import SimpleImputer
# from sklearn.pipeline import Pipeline

# from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PowerTransformer
# from sklearn.compose import ColumnTransformer, TransformedTargetRegressor

# from sklearn.svm import SVR
# from sklearn.linear_model import LinearRegression
# from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor

# from catboost import CatBoostRegressor

# from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import mean_absolute_error, mean_squared_error
import statsmodels.api as sm

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing
from statsmodels.tsa.holtwinters import ExponentialSmoothing as HoltExponentialSmoothing

"""With the data well-prepared now, the modeling can begin. A number of learning algorithms will be tested. They are:

1. ___SARIMAX___,
2. ___Exponential Smoothing___, and
3. ___ARIMA___

#### 4.1. Modelling
"""

def obtain_best(aic_values, bic_values, pq_values):
    best_aic = min(aic_values)
    best_bic = min(bic_values)

    best_ix = aic_values.index(min(aic_values))
    best_params = pq_values[best_ix]

    for i in range(len(aic_values)):
        print(f"AIC = {aic_values[i]},     BIC = {bic_values[i]},        (p, q) = {pq_values[i]}")

    return best_aic, best_bic, best_params

def optimize_exp_model(model, **kwargs):
    # Test and optimize ARIMA model
    trends = kwargs["trend"]
    damped_trends = kwargs["damped_trend"]
    seasonals = kwargs["seasonal"]
    seasonal_periods = kwargs["seasonal_periods"]
    use_boxcox = kwargs["use_boxcox"]

    aic_values = []
    bic_values = []
    pq_values = []

    for trend in trends:
        for damped_trend in damped_trends:
            for seasonal in seasonals:
                for seasonal_period in seasonal_periods:
                    for use in use_boxcox:
                        try:
                            try: #Holt-Winters
                                exp_model = model(
                                    endog = df, trend = trend, damped_trend = damped_trend,
                                    seasonal = seasonal, seasonal_periods = seasonal_period, use_boxcox = use
                                )

                            except: # Exponential
                                exp_model = model(
                                    endog = df, trend = trend, damped_trend = damped_trend,
                                    seasonal = seasonal_period,
                                )
                            try:
                                results = exp_model.fit(maxiter = kwargs["max_iter"])
                            except:
                                results = exp_model.fit()

                            r = get_metrics(results = results)

                            aic_values.append(r["AIC"])
                            bic_values.append(r["BIC"])
                            params = {
                                "trend": trend,
                                "damped_trend": damped_trend,
                                "seasonal": seasonal,
                                "seasonal_periods": seasonal_period,
                                "use_boxcox": use
                            }
                            pq_values.append(params)
                        except:
                            exp_model = None
                            pass

    seasonal_order = None

    best_aic, best_bic, best_parameters = obtain_best(aic_values, bic_values, pq_values)

    return best_aic, best_bic, best_parameters, seasonal_order

def optimize_arima_model(model, **kwargs):
    # Test and optimize ARIMA model
    p_range = q_range = list(range(kwargs["lower"], kwargs["upper"]))  # taking values from 0 to 5
    d = kwargs["d"]

    aic_values = []
    bic_values = []
    pq_values = []

    for p in p_range:
        for q in q_range:
            try:
                arima_model = model(endog = df, order = (p, d, q), )
                try:
                    seasonal_order = (p, d, q, kwargs["s"])
                    arima_model.seasonal_order = seasonal_order
                    results = arima_model.fit(max_iter = kwargs["max_iter"])
                except:
                    results = arima_model.fit()

                r = get_metrics(results = results)

                aic_values.append(r["AIC"])
                bic_values.append(r["BIC"])
                pq_values.append((p, q))
            except:
                arima_model = None
                seasonal_order = None
                pass

    best_aic, best_bic, best_parameters = obtain_best(aic_values, bic_values, pq_values)

    return best_aic, best_bic, best_parameters, seasonal_order

def optimize_model(model, **kwargs):
    try:
        return optimize_arima_model(model, **kwargs)
    except:
        return optimize_exp_model(model, **kwargs)

def get_metrics(results):
    results_ = results.summary().__str__().split("\n")
    aic = results_[4].split("AIC")[-1].strip()
    bic = results_[5].split("BIC")[-1].strip()

    return {"AIC": float(aic) if "." in aic else int(aic), "BIC": float(bic) if "." in bic else int(bic)}

# Set model keywords
arima_kwargs = {
    "lower": 0,
    "upper": 10,
    "d": 0,
    "s": 4,
    "max_iter": 500
}

"""#### SARIMAX model"""

# Get optimal parameters for SARIMAX model
sarimax_aic_, sarimax_bic_, sarimax_params, seasonal_order = optimize_model(SARIMAX, **arima_kwargs)

# Best SARIMAX model
d = 0
sarimax_p, sarimax_q = sarimax_params
try:
    sarimax_model = SARIMAX(endog = df, order=(sarimax_p, d, sarimax_q), seasonal_order = seasonal_order)
except:
    sarimax_model = SARIMAX(endog = df, order=(sarimax_p, d, sarimax_q))

sarimax_model = sarimax_model.fit(max_iter = arima_kwargs["max_iter"])

"""#### ARIMA model"""

# Get optimal parameters for ARIMA model
arima_aic, arima_bic, arima_params, _ = optimize_model(sm.tsa.arima.ARIMA, **arima_kwargs)

# Best ARIMA model
arima_p, arima_q = arima_params
arima_model = sm.tsa.arima.ARIMA(endog = df, order=(arima_p, d, arima_q))

arima_model = arima_model.fit()

"""#### Exponential Smoothing model"""

# Set model keywords
exp_kwargs = {
    "trend": ["add", "mul", "multiplicative", "additive"],
    "damped_trend": [True, False],
    "seasonal": ["add", "mul", "multiplicative", "additive"],
    "seasonal_periods": [4, 12],
    "use_boxcox": [True, False],
    "maxiter": 500
}

exp = ExponentialSmoothing(endog = df)

exp = exp.fit(max_iter = 12)

# Get optimal parameters for exponential smoothing model
exp_aic, exp_bic, exp_params, _ = optimize_model(ExponentialSmoothing, **exp_kwargs)

exp_params

# Generate parameters for ExponentialSmoothing instance
 simple_exp_params = {
    k: v for k, v in exp_params.items() if k not in ["seasonal_periods", "use_boxcox"]
}
simple_exp_params["seasonal"] = exp_params["seasonal_periods"]

# Best ExponentialSmoothing model
exp_model = ExponentialSmoothing(endog = df, **simple_exp_params)

exp_model = exp_model.fit(maxiter = exp_kwargs["maxiter"])

"""#### Holt Exponential Smoothing model"""

# Get optimal parameters for Holt exponential smoothing model
holt_aic, holt_bic, holt_params, _ = optimize_model(HoltExponentialSmoothing, **exp_kwargs)

holt_params

# Best HoltExponentialSmoothing model
holt_model = HoltExponentialSmoothing(endog = df, **holt_params)

holt_model = holt_model.fit()

"""<div align="center"><h3>4.1. Enter Data Modelling Code Here</h3></div>

---

<div><h3>05. Model Evaluation</h3></div>

The trained model is in need of evaluation. The main metrics of interest are ___R2___, ___MAPE___, ___MAE___ and ___MSE___. This is due to the fact that the problem is a regression problem.

<div align="center"><h3>5.1. Enter Model Evaluation Code Here</h3></div>
"""

# Mean absolute error implementation
def mean_absolute_percentage_error(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    mape = np.mean(np.abs(y_true - y_pred) / (y_true + 1)) * 100
    return mape

def visualize_results(model_name, df, predictions, num_steps = 100, first_timestep = True):

    if first_timestep:
        x, y = list(map(lambda x: x.strftime("%Y-%m-%d"), df.index[:num_steps].tolist())), list(df.price_usd)[:num_steps]
        y_ = list(predictions)[:num_steps]
        position = "First"
    else:
        x, y = list(map(lambda x: x.strftime("%Y-%m-%d"), df.index[-num_steps:].tolist())), list(df.price_usd)[-num_steps:]
        y_ = list(predictions)[-num_steps:]
        position = "Last"

    title = f"Actual and Predicted Prices via {model_name} in U.S. Dollars ({position} {num_steps} timesteps)"

    plt.figure(figsize=(20, 7.5))
    plt.plot(x, y, label="Actual")
    plt.plot(x, y_, 'r', label="Predicted")

    plt.xticks(rotation = 90)

    plt.xlabel("Time (in Days)")
    plt.ylabel("Price (US Dollars)")
    plt.title(title)

    plt.legend()

    # plt.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8))
    plt.show(); plt.close()

    return

def evaluate_model(model, df, num_steps = 100, first_timestep = True):
    model_name = model.__class__.__name__[:-14]
    print(f"Evaluating {model_name} model...\n")

    predictions = model.predict(start=0, end=len(df)-1)

    mse = mean_squared_error(list(df.price_usd), list(predictions))
    print("Mean Squared Error:", mse)

    mape = mean_absolute_percentage_error(list(df.price_usd), list(predictions))
    print("Mean Absolute Percentage Error:", mape)

    mae = mean_absolute_error(list(df.price_usd), list(predictions))
    print("Mean Absolute Error:", mae)

    visualize_results(model_name, df, predictions, num_steps, first_timestep)

    return

evaluate_model(arima_model, df)

evaluate_model(arima_model, df, first_timestep = False)

evaluate_model(sarimax_model, df)

evaluate_model(sarimax_model, df, first_timestep = False)

evaluate_model(holt_model, df)

evaluate_model(holt_model, df, first_timestep = False)

evaluate_model(exp_model, df)

evaluate_model(exp_model, df, first_timestep = False)

"""#### 5.3 Performance Analysis

Based on the visualizations and metrics observed from training, it would seem that the _SARIMAX_ model exhibits the most promise based on the metric values, but shows some issues in practice. Over time the fit worsens. However, it would seem that the main issue with the fit is the lag between actual and predicted cryptocurrency prices. This would imply that the SARIMAX model learns the data patterns quite well. More data would likely improve model performance.

The _ARIMA_ model seems to follow the same pattern as the _SARIMAX_, even though both the metric values and the fit seem a little worse. The simple _Exponential Smoothing_ model seems to start out with bad fit, but at the end, the fit improves dramatically, even though it still seems to fall behind the _ARIMA_ and _SARIMAX_ models. The _Holt-Winters exponential smoothing_ model seems to follow the same trend, though it is more stable than the _Exponential Smoothing_ model.


Based on this analysis, the model of choice is the _SARIMAX_ model. This is because:

1. It shows the best promise with respect to the metrics of interest, and
2. It also shows the best promise of being improved with the addition of more data.

---

<div><h3>06. Model Deployment</h3></div>

With the model performance up to speed, the final artefact needs to be deployed for use. There are a number of options. They include:


1. Local Deployment
   * Flask
   * Django
2. Cloud Deployment
   * AWS
   * GCP
   * Streamlit + GitHub

<div align="center"><h3>6.1. Model Deployment Code</h3></div>
"""



